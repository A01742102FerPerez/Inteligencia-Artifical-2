{"cells":[{"cell_type":"markdown","id":"a8d06576","metadata":{},"source":["# Using Pre-trained Word Embeddings\n","\n","In this notebook we will show some operations on pre-trained word embeddings to gain an intuition about them.\n","\n","We will be using the pre-trained GloVe embeddings that can be found in the [official website](https://nlp.stanford.edu/projects/glove/). In particular, we will use the file `glove.6B.300d.txt` contained in this [zip file](https://nlp.stanford.edu/data/glove.6B.zip).\n","\n","We will first load the GloVe embeddings using [Gensim](https://radimrehurek.com/gensim/). Specifically, we will use [`KeyedVectors`](https://radimrehurek.com/gensim/models/keyedvectors.html)'s [`load_word2vec_format()`](https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.KeyedVectors.load_word2vec_format) classmethod, which supports the original word2vec file format.\n","However, there is a difference in the file formats used by GloVe and word2vec, which is a header used by word2vec to indicate the number of embeddings and dimensions stored in the file. The file that stores the GloVe embeddings doesn't have this header, so we will have to address that when loading the embeddings.\n","\n","Loading the embeddings may take a little bit, so hang in there!"]},{"cell_type":"code","execution_count":3,"id":"80c783ba","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting gensim\n","  Downloading gensim-4.2.0-cp37-cp37m-win_amd64.whl.metadata (8.6 kB)\n","Requirement already satisfied: numpy>=1.17.0 in c:\\users\\usuario\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from gensim) (1.21.6)\n","Requirement already satisfied: scipy>=0.18.1 in c:\\users\\usuario\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from gensim) (1.7.3)\n","Collecting smart-open>=1.8.1 (from gensim)\n","  Downloading smart_open-7.0.5-py3-none-any.whl.metadata (24 kB)\n","Collecting Cython==0.29.28 (from gensim)\n","  Downloading Cython-0.29.28-py2.py3-none-any.whl.metadata (2.8 kB)\n","Requirement already satisfied: wrapt in c:\\users\\usuario\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from smart-open>=1.8.1->gensim) (1.16.0)\n","Downloading gensim-4.2.0-cp37-cp37m-win_amd64.whl (24.0 MB)\n","   ---------------------------------------- 24.0/24.0 MB 5.7 MB/s eta 0:00:00\n","Downloading Cython-0.29.28-py2.py3-none-any.whl (983 kB)\n","   ---------------------------------------- 983.8/983.8 kB 5.2 MB/s eta 0:00:00\n","Downloading smart_open-7.0.5-py3-none-any.whl (61 kB)\n","   ---------------------------------------- 61.4/61.4 kB 1.6 MB/s eta 0:00:00\n","Installing collected packages: smart-open, Cython, gensim\n","  Attempting uninstall: Cython\n","    Found existing installation: Cython 3.0.8\n","    Uninstalling Cython-3.0.8:\n","      Successfully uninstalled Cython-3.0.8\n","Successfully installed Cython-0.29.28 gensim-4.2.0 smart-open-7.0.5\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["pip install gensim"]},{"cell_type":"code","execution_count":4,"id":"10ba3116","metadata":{},"outputs":[{"data":{"text/plain":["(400000, 300)"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["from gensim.models import KeyedVectors\n","\n","fname = \"glove.6B.300d.txt\"\n","glove = KeyedVectors.load_word2vec_format(fname, no_header=True)\n","glove.vectors.shape"]},{"cell_type":"markdown","id":"c860fb18","metadata":{},"source":["## Word similarity\n","\n","One attribute of word embeddings that makes them useful is the ability to compare them using cosine similarity to find how similar they are. [`KeyedVectors`](https://radimrehurek.com/gensim/models/keyedvectors.html) objects provide a method called [`most_similar()`](https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.KeyedVectors.most_similar) that we can use to find the closest words to a particular word of interest. By default, [`most_similar()`](https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.KeyedVectors.most_similar) returns the 10 most similar words, but this can be changed using the `topn` parameter.\n","\n","Below we test this function using a few different words."]},{"cell_type":"code","execution_count":5,"id":"8ed2a4ac","metadata":{"execution":{"iopub.execute_input":"2024-10-14T20:40:59.635513Z","iopub.status.busy":"2024-10-14T20:40:59.634994Z","iopub.status.idle":"2024-10-14T20:40:59.658035Z","shell.execute_reply":"2024-10-14T20:40:59.656573Z","shell.execute_reply.started":"2024-10-14T20:40:59.635444Z"},"trusted":true},"outputs":[{"data":{"text/plain":["[('cacti', 0.663456380367279),\n"," ('saguaro', 0.6195855140686035),\n"," ('pear', 0.5233485698699951),\n"," ('cactuses', 0.5178281664848328),\n"," ('prickly', 0.515631914138794),\n"," ('mesquite', 0.48448556661605835),\n"," ('opuntia', 0.4540084898471832),\n"," ('shrubs', 0.45362064242362976),\n"," ('peyote', 0.45344963669776917),\n"," ('succulents', 0.4512787461280823)]"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["# common noun\n","glove.most_similar(\"cactus\")"]},{"cell_type":"code","execution_count":6,"id":"14c1f94c","metadata":{"execution":{"iopub.execute_input":"2024-10-14T20:41:02.504028Z","iopub.status.busy":"2024-10-14T20:41:02.503614Z","iopub.status.idle":"2024-10-14T20:41:02.526066Z","shell.execute_reply":"2024-10-14T20:41:02.524663Z","shell.execute_reply.started":"2024-10-14T20:41:02.503989Z"},"trusted":true},"outputs":[{"data":{"text/plain":["[('cakes', 0.7506030201911926),\n"," ('chocolate', 0.6965583562850952),\n"," ('dessert', 0.6440261006355286),\n"," ('pie', 0.608742892742157),\n"," ('cookies', 0.6082394123077393),\n"," ('frosting', 0.601721465587616),\n"," ('bread', 0.5954801440238953),\n"," ('cookie', 0.593381941318512),\n"," ('recipe', 0.5827102661132812),\n"," ('baked', 0.5819962620735168)]"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["# common noun\n","glove.most_similar(\"cake\")"]},{"cell_type":"code","execution_count":7,"id":"a9d4a84c","metadata":{"execution":{"iopub.execute_input":"2024-10-14T20:40:54.137547Z","iopub.status.busy":"2024-10-14T20:40:54.137106Z","iopub.status.idle":"2024-10-14T20:40:54.159240Z","shell.execute_reply":"2024-10-14T20:40:54.157679Z","shell.execute_reply.started":"2024-10-14T20:40:54.137505Z"},"trusted":true},"outputs":[{"data":{"text/plain":["[('enraged', 0.7087873816490173),\n"," ('furious', 0.7078357934951782),\n"," ('irate', 0.6938743591308594),\n"," ('outraged', 0.6705204248428345),\n"," ('frustrated', 0.6515549421310425),\n"," ('angered', 0.635320246219635),\n"," ('provoked', 0.5827428102493286),\n"," ('annoyed', 0.581898033618927),\n"," ('incensed', 0.5751833319664001),\n"," ('indignant', 0.5704443454742432)]"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["# adjective\n","glove.most_similar(\"angry\")"]},{"cell_type":"code","execution_count":9,"id":"f8c51baa","metadata":{"execution":{"iopub.execute_input":"2024-10-14T20:41:07.278940Z","iopub.status.busy":"2024-10-14T20:41:07.278156Z","iopub.status.idle":"2024-10-14T20:41:07.303538Z","shell.execute_reply":"2024-10-14T20:41:07.301964Z","shell.execute_reply.started":"2024-10-14T20:41:07.278893Z"},"trusted":true},"outputs":[{"data":{"text/plain":["[('soon', 0.766185998916626),\n"," ('rapidly', 0.7216640114784241),\n"," ('swiftly', 0.7197349667549133),\n"," ('eventually', 0.7043026685714722),\n"," ('finally', 0.6900882124900818),\n"," ('immediately', 0.6842609643936157),\n"," ('then', 0.6697486042976379),\n"," ('slowly', 0.6645645499229431),\n"," ('gradually', 0.6401675939559937),\n"," ('when', 0.6347666382789612)]"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["# adverb\n","glove.most_similar(\"quickly\")"]},{"cell_type":"code","execution_count":10,"id":"4860aa5e","metadata":{"execution":{"iopub.execute_input":"2024-10-14T20:41:08.868824Z","iopub.status.busy":"2024-10-14T20:41:08.867884Z","iopub.status.idle":"2024-10-14T20:41:08.891031Z","shell.execute_reply":"2024-10-14T20:41:08.889582Z","shell.execute_reply.started":"2024-10-14T20:41:08.868778Z"},"trusted":true},"outputs":[{"data":{"text/plain":["[('sides', 0.5867610573768616),\n"," ('both', 0.5843431949615479),\n"," ('two', 0.5652360916137695),\n"," ('differences', 0.514071524143219),\n"," ('which', 0.5120179057121277),\n"," ('conflict', 0.5115456581115723),\n"," ('relationship', 0.5022751092910767),\n"," ('and', 0.498425155878067),\n"," ('in', 0.4970666766166687),\n"," ('relations', 0.4970114529132843)]"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["# preposition\n","glove.most_similar(\"between\")"]},{"cell_type":"code","execution_count":11,"id":"71ebb7d8","metadata":{},"outputs":[{"data":{"text/plain":["[('of', 0.7057957649230957),\n"," ('which', 0.6992015838623047),\n"," ('this', 0.6747026443481445),\n"," ('part', 0.6727458238601685),\n"," ('same', 0.6592389345169067),\n"," ('its', 0.6446539759635925),\n"," ('first', 0.6398990750312805),\n"," ('in', 0.6361348032951355),\n"," ('one', 0.6245334148406982),\n"," ('that', 0.6176422834396362)]"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["# determiner\n","glove.most_similar(\"the\")"]},{"cell_type":"markdown","id":"3ca6087f","metadata":{},"source":["## Word analogies\n","\n","Another characteristic of word embeddings is their ability to solve analogy problems.\n","The same [`most_similar()`](https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.KeyedVectors.most_similar) method can be used for this task, by passing two lists of words:\n","a `positive` list with the words that should be added and a `negative` list with the words that should be subtracted. Using these arguments, the famous example $\\vec{king} - \\vec{man} + \\vec{woman} \\approx \\vec{queen}$ can be executed as follows:"]},{"cell_type":"code","execution_count":12,"id":"0b0aff13","metadata":{"execution":{"iopub.execute_input":"2024-10-14T20:40:47.775519Z","iopub.status.busy":"2024-10-14T20:40:47.775015Z","iopub.status.idle":"2024-10-14T20:40:47.799547Z","shell.execute_reply":"2024-10-14T20:40:47.797955Z","shell.execute_reply.started":"2024-10-14T20:40:47.775459Z"},"trusted":true},"outputs":[{"data":{"text/plain":["[('queen', 0.6713277101516724),\n"," ('princess', 0.5432624816894531),\n"," ('throne', 0.5386103987693787),\n"," ('monarch', 0.5347574949264526),\n"," ('daughter', 0.49802514910697937),\n"," ('mother', 0.49564430117607117),\n"," ('elizabeth', 0.4832652509212494),\n"," ('kingdom', 0.47747090458869934),\n"," ('prince', 0.4668239951133728),\n"," ('wife', 0.46473270654678345)]"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["# king - man + woman\n","glove.most_similar(positive=[\"king\", \"woman\"], negative=[\"man\"])"]},{"cell_type":"markdown","id":"b2708d71","metadata":{},"source":["Here are a few other interesting analogies:"]},{"cell_type":"code","execution_count":13,"id":"0ed85626","metadata":{},"outputs":[{"data":{"text/plain":["[('airplane', 0.5897148251533508),\n"," ('flying', 0.5675230026245117),\n"," ('plane', 0.5317023992538452),\n"," ('flies', 0.5172374248504639),\n"," ('flown', 0.514790415763855),\n"," ('airplanes', 0.5091356635093689),\n"," ('flew', 0.5011662244796753),\n"," ('planes', 0.4970923364162445),\n"," ('aircraft', 0.4957723915576935),\n"," ('helicopter', 0.45859551429748535)]"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["# car - drive + fly\n","glove.most_similar(positive=[\"car\", \"fly\"], negative=[\"drive\"])"]},{"cell_type":"code","execution_count":14,"id":"60cdd963","metadata":{"execution":{"iopub.execute_input":"2024-10-14T20:41:15.417540Z","iopub.status.busy":"2024-10-14T20:41:15.417143Z","iopub.status.idle":"2024-10-14T20:41:15.440746Z","shell.execute_reply":"2024-10-14T20:41:15.439283Z","shell.execute_reply.started":"2024-10-14T20:41:15.417473Z"},"trusted":true},"outputs":[{"data":{"text/plain":["[('sydney', 0.6780862212181091),\n"," ('melbourne', 0.6499180793762207),\n"," ('australian', 0.594883143901825),\n"," ('perth', 0.5828553438186646),\n"," ('canberra', 0.5610732436180115),\n"," ('brisbane', 0.5523110628128052),\n"," ('zealand', 0.5240115523338318),\n"," ('queensland', 0.5193883180618286),\n"," ('adelaide', 0.5027671456336975),\n"," ('london', 0.4644604027271271)]"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["# berlin - germany + australia\n","glove.most_similar(positive=[\"berlin\", \"australia\"], negative=[\"germany\"])"]},{"cell_type":"code","execution_count":15,"id":"6587450f","metadata":{"execution":{"iopub.execute_input":"2024-10-14T20:40:38.755140Z","iopub.status.busy":"2024-10-14T20:40:38.754733Z","iopub.status.idle":"2024-10-14T20:40:38.779812Z","shell.execute_reply":"2024-10-14T20:40:38.778395Z","shell.execute_reply.started":"2024-10-14T20:40:38.755102Z"},"trusted":true},"outputs":[{"data":{"text/plain":["[('iraq', 0.5320571660995483),\n"," ('fallujah', 0.4834090769290924),\n"," ('iraqi', 0.47287362813949585),\n"," ('mosul', 0.464663565158844),\n"," ('iraqis', 0.43555372953414917),\n"," ('najaf', 0.4352763295173645),\n"," ('baqouba', 0.42063194513320923),\n"," ('basra', 0.41905173659324646),\n"," ('samarra', 0.4125366508960724),\n"," ('saddam', 0.40791556239128113)]"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["# england - london + baghdad\n","glove.most_similar(positive=[\"england\", \"baghdad\"], negative=[\"london\"])"]},{"cell_type":"code","execution_count":16,"id":"3daedb41","metadata":{"execution":{"iopub.execute_input":"2024-10-14T20:40:42.639916Z","iopub.status.busy":"2024-10-14T20:40:42.638976Z","iopub.status.idle":"2024-10-14T20:40:42.662396Z","shell.execute_reply":"2024-10-14T20:40:42.660998Z","shell.execute_reply.started":"2024-10-14T20:40:42.639868Z"},"trusted":true},"outputs":[{"data":{"text/plain":["[('mexico', 0.5726832151412964),\n"," ('philippines', 0.5445368885993958),\n"," ('peru', 0.48382261395454407),\n"," ('venezuela', 0.4816672205924988),\n"," ('brazil', 0.4664309620857239),\n"," ('argentina', 0.45490506291389465),\n"," ('philippine', 0.4417841136455536),\n"," ('chile', 0.43960973620414734),\n"," ('colombia', 0.4386259913444519),\n"," ('thailand', 0.43396785855293274)]"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["# japan - yen + peso\n","glove.most_similar(positive=[\"japan\", \"peso\"], negative=[\"yen\"])"]},{"cell_type":"code","execution_count":17,"id":"fcbb2288","metadata":{"execution":{"iopub.execute_input":"2024-10-14T20:41:21.585842Z","iopub.status.busy":"2024-10-14T20:41:21.585127Z","iopub.status.idle":"2024-10-14T20:41:21.609205Z","shell.execute_reply":"2024-10-14T20:41:21.607890Z","shell.execute_reply.started":"2024-10-14T20:41:21.585789Z"},"trusted":true},"outputs":[{"data":{"text/plain":["[('tallest', 0.5077419281005859),\n"," ('taller', 0.47616496682167053),\n"," ('height', 0.46000057458877563),\n"," ('metres', 0.4584786891937256),\n"," ('cm', 0.45212721824645996),\n"," ('meters', 0.44067248702049255),\n"," ('towering', 0.42784252762794495),\n"," ('centimeters', 0.4234543442726135),\n"," ('inches', 0.41745859384536743),\n"," ('erect', 0.4087314009666443)]"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["# best - good + tall\n","glove.most_similar(positive=[\"best\", \"tall\"], negative=[\"good\"])"]},{"cell_type":"markdown","id":"44c1f235","metadata":{},"source":["## Looking under the hood\n","\n","Now that we are more familiar with the [`most_similar()`](https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.KeyedVectors.most_similar) method, it is time to implement its functionality ourselves.\n","But first, we need to take a look at the different parts of the [`KeyedVectors`](https://radimrehurek.com/gensim/models/keyedvectors.html) object that we will need.\n","Obviously, we will need the vectors themselves. They are stored in the `vectors` attribute."]},{"cell_type":"code","execution_count":18,"id":"948f8af6","metadata":{},"outputs":[{"data":{"text/plain":["(400000, 300)"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["glove.vectors.shape"]},{"cell_type":"markdown","id":"24946fc1","metadata":{},"source":["As we can see above, `vectors` is a 2-dimensional matrix with 400,000 rows and 300 columns.\n","Each row corresponds to a 300-dimensional word embedding. These embeddings are not normalized, but normalized embeddings can be obtained using the [`get_normed_vectors()`](https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.KeyedVectors.get_normed_vectors) method."]},{"cell_type":"code","execution_count":19,"id":"dfaa8d95","metadata":{},"outputs":[{"data":{"text/plain":["(400000, 300)"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["normed_vectors = glove.get_normed_vectors()\n","normed_vectors.shape"]},{"cell_type":"markdown","id":"0133dae2","metadata":{},"source":["Now we need to map the words in the vocabulary to rows in the `vectors` matrix, and vice versa.\n","The [`KeyedVectors`](https://radimrehurek.com/gensim/models/keyedvectors.html) object has the attributes `index_to_key` and `key_to_index` which are a list of words and a dictionary of words to indices, respectively."]},{"cell_type":"code","execution_count":20,"id":"f746dfeb","metadata":{},"outputs":[],"source":["#glove.index_to_key"]},{"cell_type":"code","execution_count":21,"id":"c0edf76e","metadata":{},"outputs":[],"source":["#glove.key_to_index"]},{"cell_type":"markdown","id":"c96e0979","metadata":{},"source":["## Word similarity from scratch\n","\n","Now we have everything we need to implement a `most_similar_words()` function that takes a word, the vector matrix, the `index_to_key` list, and the `key_to_index` dictionary. This function will return the 10 most similar words to the provided word, along with their similarity scores."]},{"cell_type":"code","execution_count":24,"id":"a8f35262","metadata":{},"outputs":[],"source":["import numpy as np\n","\n","def most_similar_words(word, vectors, index_to_key, key_to_index, topn=10):\n","    # retrieve word_id corresponding to given word\n","    word_id = key_to_index[word]\n","    \n","    # retrieve embedding for given word\n","    word_vec = vectors[word_id]\n","    \n","    # calculate similarities to all words in vocabulary (hint: use @ for dot product)\n","    similarities = vectors @ word_vec\n","    \n","    # get word_ids in ascending order with respect to similarity score\n","    word_ids = np.argsort(similarities)\n","    \n","    # reverse word_ids \n","    word_ids = word_ids[::-1]\n","    \n","    # get boolean array with element corresponding to word_id set to false\n","    # obtain new array of indices that doesn't contain word_id\n","    # (otherwise the most similar word to the argument would be the argument itself)\n","    word_ids = word_ids[word_ids != word_id]\n","    \n","    # get topn word_ids\n","    top_word_ids = word_ids[:topn]\n","    \n","    # retrieve topn words with their corresponding similarity score\n","    top_words = [(index_to_key[idx], similarities[idx]) for idx in top_word_ids]\n","    \n","    # return results\n","    return top_words\n"]},{"cell_type":"code","execution_count":22,"id":"746ade55","metadata":{},"outputs":[],"source":["\"\"\"import numpy as np\n","\n","def most_similar_words(word, vectors, index_to_key, key_to_index, topn=10):\n","    # retrieve word_id corresponding to given word\n","    \n","    # retrieve embedding for given word\n","    \n","    # calculate similarities to all words in out vocabulary (hint: use @)\n","    \n","    # get word_ids in ascending order with respect to similarity score\n","    \n","    # reverse word_ids\n","    \n","    # get boolean array with element corresponding to word_id set to false\n","    \n","    # obtain new array of indices that doesn't contain word_id\n","    # (otherwise the most similar word to the argument would be the argument itself)\n","    \n","    # get topn word_ids\n","    \n","    # retrieve topn words with their corresponding similarity score\n","    \n","    # return results\n","    return top_words\"\"\""]},{"cell_type":"markdown","id":"d26181c6","metadata":{},"source":["Now let's try the same example that we used above: the most similar words to \"cactus\"."]},{"cell_type":"code","execution_count":25,"id":"f878a162","metadata":{"execution":{"iopub.execute_input":"2024-10-14T19:34:48.069448Z","iopub.status.busy":"2024-10-14T19:34:48.069146Z","iopub.status.idle":"2024-10-14T19:34:48.355878Z","shell.execute_reply":"2024-10-14T19:34:48.354644Z","shell.execute_reply.started":"2024-10-14T19:34:48.069415Z"},"trusted":true},"outputs":[{"data":{"text/plain":["[('cacti', 0.66345644),\n"," ('saguaro', 0.6195854),\n"," ('pear', 0.5233487),\n"," ('cactuses', 0.5178282),\n"," ('prickly', 0.51563185),\n"," ('mesquite', 0.4844855),\n"," ('opuntia', 0.45400843),\n"," ('shrubs', 0.45362067),\n"," ('peyote', 0.4534496),\n"," ('succulents', 0.45127875)]"]},"execution_count":25,"metadata":{},"output_type":"execute_result"}],"source":["vectors = glove.get_normed_vectors()\n","index_to_key = glove.index_to_key\n","key_to_index = glove.key_to_index\n","most_similar_words(\"cactus\", vectors, index_to_key, key_to_index)"]},{"cell_type":"markdown","id":"212306f0","metadata":{},"source":["## Analogies from scratch\n","\n","The `most_similar_words()` function behaves as expected. Now let's implement a function to perform the analogy task. We will give it the very creative name `analogy`. This function will get two lists of words (one for positive words and one for negative words), just like the [`most_similar()`](https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.KeyedVectors.most_similar) method we discussed above."]},{"cell_type":"code","execution_count":26,"id":"f8cbf805","metadata":{},"outputs":[],"source":["from numpy.linalg import norm\n","import numpy as np\n","\n","def analogy(positive, negative, vectors, index_to_key, key_to_index, topn=10):\n","    # find ids for positive and negative words\n","    pos_ids = [key_to_index[word] for word in positive]\n","    neg_ids = [key_to_index[word] for word in negative]\n","    given_word_ids = pos_ids + neg_ids\n","    \n","    # get embeddings for positive and negative words\n","    pos_emb = np.sum([vectors[word_id] for word_id in pos_ids], axis=0)\n","    neg_emb = np.sum([vectors[word_id] for word_id in neg_ids], axis=0)\n","    \n","    # get embedding for analogy \n","    emb = pos_emb - neg_emb\n","    \n","    # normalize embedding\n","    emb = emb / norm(emb)\n","    \n","    # calculate similarities to all words in the vocabulary\n","    similarities = vectors @ emb\n","    \n","    # get word_ids in ascending order with respect to similarity score\n","    ids_ascending = np.argsort(similarities)\n","    \n","    # reverse word_ids \n","    ids_descending = ids_ascending[::-1]\n","    \n","    # get boolean array with element corresponding to any of given_word_ids set to false\n","    ###Hint: You can use np.isni \n","    given_words_mask = np.isin(ids_descending, given_word_ids, invert=True)\n","    \n","    # obtain new array of indices that doesn't contain any of the given_word_ids\n","    ids_descending = ids_descending[given_words_mask]\n","    \n","    # get topn word_ids\n","    top_ids = ids_descending[:topn]\n","    \n","    # retrieve topn words with their corresponding similarity score\n","    top_words = [(index_to_key[idx], similarities[idx]) for idx in top_ids]\n","    \n","    # return results\n","    return top_words\n"]},{"cell_type":"code","execution_count":21,"id":"1150f380","metadata":{},"outputs":[],"source":["\"\"\"from numpy.linalg import norm\n","\n","def analogy(positive, negative, vectors, index_to_key, key_to_index, topn=10):\n","    # find ids for positive and negative words\n","    pos_ids = \n","    neg_ids = \n","    given_word_ids = pos_ids + neg_ids\n","    # get embeddings for positive and negative words\n","    pos_emb = \n","    neg_emb = \n","    # get embedding for analogy\n","    emb = \n","    # normalize embedding\n","    emb = \n","    # calculate similarities to all words in out vocabulary\n","    similarities = \n","    # get word_ids in ascending order with respect to similarity score\n","    ids_ascending = \n","    # reverse word_ids\n","    ids_descending = \n","    # get boolean array with element corresponding to any of given_word_ids set to false\n","    ###Hint: You can use np.isni \n","    given_words_mask = \n","    # obtain new array of indices that doesn't contain any of the given_word_ids\n","    ids_descending = i\n","    # get topn word_ids\n","    top_ids = \n","    # retrieve topn words with their corresponding similarity score\n","    top_words = \n","    # return results\n","    return top_words\"\"\""]},{"cell_type":"markdown","id":"26664aed","metadata":{},"source":["Let's try this function with the $\\vec{king} - \\vec{man} + \\vec{woman} \\approx \\vec{queen}$ example we discussed above."]},{"cell_type":"code","execution_count":27,"id":"94cbc6ab","metadata":{"execution":{"iopub.execute_input":"2024-10-14T20:40:30.924531Z","iopub.status.busy":"2024-10-14T20:40:30.923985Z","iopub.status.idle":"2024-10-14T20:40:30.961378Z","shell.execute_reply":"2024-10-14T20:40:30.960104Z","shell.execute_reply.started":"2024-10-14T20:40:30.924439Z"},"trusted":true},"outputs":[{"data":{"text/plain":["[('queen', 0.67132777),\n"," ('princess', 0.5432625),\n"," ('throne', 0.5386105),\n"," ('monarch', 0.53475755),\n"," ('daughter', 0.49802518),\n"," ('mother', 0.49564433),\n"," ('elizabeth', 0.48326525),\n"," ('kingdom', 0.47747087),\n"," ('prince', 0.466824),\n"," ('wife', 0.4647328)]"]},"execution_count":27,"metadata":{},"output_type":"execute_result"}],"source":["positive = [\"king\", \"woman\"]\n","negative = [\"man\"]\n","vectors = glove.get_normed_vectors()\n","index_to_key = glove.index_to_key\n","key_to_index = glove.key_to_index\n","analogy(positive, negative, vectors, index_to_key, key_to_index)"]},{"cell_type":"code","execution_count":null,"id":"bf96bca0","metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30786,"isGpuEnabled":false,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.9"}},"nbformat":4,"nbformat_minor":5}
